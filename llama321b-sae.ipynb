{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "53e7bd62",
      "metadata": {
        "id": "53e7bd62"
      },
      "source": [
        "# Sparse Autoencoders: Interpreting the Llama-3.2-1B Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13fbf19",
      "metadata": {
        "id": "d13fbf19"
      },
      "outputs": [],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a5e3605",
      "metadata": {
        "id": "4a5e3605"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorWithPadding, pipeline\n",
        "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
        "from huggingface_hub import notebook_login\n",
        "import tqdm\n",
        "import heapq\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f2e773",
      "metadata": {
        "id": "b7f2e773"
      },
      "outputs": [],
      "source": [
        "# llama 3.2-1B is a gated model, so we need to login to use it with transformers\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc7094d",
      "metadata": {
        "id": "2fc7094d"
      },
      "outputs": [],
      "source": [
        "# loading stuff here\n",
        "try:\n",
        "    model_path = '../Llama-3.2-1B-Instruct'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map='auto', return_dict_in_generate=True, output_hidden_states=True)\n",
        "except:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", torch_dtype=torch.float16, device_map='auto', return_dict_in_generate=True, output_hidden_states=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ccf29d8",
      "metadata": {
        "id": "0ccf29d8"
      },
      "outputs": [],
      "source": [
        "# Testing the Llama model\n",
        "\n",
        "inputs = tokenizer('Hello LLaMa!', return_tensors='pt').to(model.device)\n",
        "# input_ids = tokenizer('Hello LLaMa!', return_tensors='pt').input_ids.to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    z = outputs.hidden_states[-1]\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377a3e63",
      "metadata": {
        "id": "377a3e63"
      },
      "outputs": [],
      "source": [
        "print(generated_ids)\n",
        "print(generated_text)\n",
        "\n",
        "print(z.shape)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78884ff6",
      "metadata": {
        "id": "78884ff6"
      },
      "outputs": [],
      "source": [
        "# simple encoder and decoder modules for the SAE\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dtype, activation_fn=F.leaky_relu):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc = nn.Linear(in_dim, out_dim, bias=True, dtype=dtype)\n",
        "        self.activation_fn = activation_fn\n",
        "        nn.init.kaiming_uniform_(self.enc.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: b, L, in_dim\n",
        "        # returns h(z): b, L, out_dim\n",
        "        return self.activation_fn(self.enc.forward(z))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dtype):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec = nn.Linear(in_dim, out_dim, bias=True, dtype=dtype)\n",
        "\n",
        "    def forward(self, hz):\n",
        "        # hz: b, L, in_dim\n",
        "        # returns zhat: b, L, out_dim\n",
        "        return self.dec.forward(hz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b6b6bb",
      "metadata": {
        "id": "51b6b6bb"
      },
      "outputs": [],
      "source": [
        "# standard SAE implementation\n",
        "\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, feature_dim, sparse_dim, alpha, dtype=torch.float16):\n",
        "        super(SAE, self).__init__()\n",
        "        self.E = Encoder(feature_dim, sparse_dim, dtype)\n",
        "        self.D = Decoder(sparse_dim, feature_dim, dtype)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: b, L, feature_dim\n",
        "        # returns zhat: b, L, feature_dim\n",
        "        # returns hz: b, L, sparse_dim\n",
        "        hz = self.E.forward(z)\n",
        "        zhat = self.D.forward(hz)\n",
        "        return zhat, hz\n",
        "\n",
        "    def loss(self, z, zhat, hz, attention_mask):\n",
        "        # reconstruction_loss = torch.square(torch.norm(z - zhat, p=2))\n",
        "        attention_mask = torch.unsqueeze(attention_mask, dim=-1)\n",
        "        # print(zhat.shape, z.shape, attention_mask.shape)\n",
        "        reconstruction_loss = F.mse_loss(zhat * attention_mask, z * attention_mask)\n",
        "        sparsity_regularization = self.alpha * torch.norm(hz, p=1)\n",
        "        return reconstruction_loss + sparsity_regularization\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7349ff",
      "metadata": {
        "id": "1c7349ff"
      },
      "outputs": [],
      "source": [
        "# Custom DataLoader\n",
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        # dataset is a dictionary containing 'input_ids': [tensors]\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset['input_ids'][idx]\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        input_ids = pad_sequence(data, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
        "        attention_mask = torch.where(input_ids != tokenizer.pad_token_id, 1, 0).to(device)\n",
        "        return { 'input_ids': input_ids, 'attention_mask': attention_mask }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81877c93",
      "metadata": {
        "id": "81877c93"
      },
      "outputs": [],
      "source": [
        "# Load Llama Nemotron dataset\n",
        "try:\n",
        "    dataset = load_dataset('../Llama-Nemotron-Post-Training-Dataset/SFT/chat', split='train').with_format('torch')\n",
        "except:\n",
        "    dataset = load_dataset('nvidia/Llama-Nemotron-Post-Training-Dataset', 'SFT', data_dir='SFT/chat').with_format('torch')['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47378c4",
      "metadata": {
        "id": "f47378c4"
      },
      "outputs": [],
      "source": [
        "def tokenize_raw_data(x):\n",
        "    input_text = [ex[0]['content'] for ex in x['input']]\n",
        "    input_ids = tokenizer(input_text)\n",
        "    # input_ids['output_ids'] = tokenizer(x['output'])['input_ids'] # Uncomment if we need output ids\n",
        "    return input_ids\n",
        "\n",
        "# trim for performance\n",
        "trim_dataset = dataset.train_test_split(test_size=0.9)['train']\n",
        "trim_dataset = dataset.filter(lambda sample: len(sample['input'][0]['content']) <= 50)\n",
        "\n",
        "# dataset keys: input, output, category, license, reasoning, generator, used_in_training, version, system_prompt\n",
        "encoded_dataset = trim_dataset.map(tokenize_raw_data, batched=True) # added input_ids, attention_mask (for input), and (maybe) output_ids\n",
        "\n",
        "# retrieve just tokenized data\n",
        "samples = { k : encoded_dataset[k] for k in encoded_dataset.features if k in [ 'input_ids' ] } # attention mask is all 1s of same size tensor as input_ids, so don't need to store it\n",
        "\n",
        "# dataloader = DataLoader(samples, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
        "tokenized_dataset = TokenizedDataset(samples)\n",
        "dataloader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True, collate_fn=tokenized_dataset.collate_fn)\n",
        "\n",
        "# for data in dataloader:\n",
        "#     output = model(**data)\n",
        "#     print(data['input_ids'].shape)\n",
        "#     print(output.hidden_states[-1])\n",
        "#     print(output.hidden_states[-1].shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4dfce56",
      "metadata": {
        "id": "a4dfce56"
      },
      "outputs": [],
      "source": [
        "# Train function\n",
        "def train(llm, sae, dataloader, epochs, optimizer):\n",
        "    for epoch in tqdm.trange(epochs, desc=\"training\", unit=\"epoch\"):\n",
        "        with tqdm.tqdm(dataloader, desc=f\"epoch {epoch + 1}\", unit=\"batch\", total=len(dataloader), position=0, leave=True) as batch_iterator:\n",
        "            sae.train()\n",
        "            total_loss = 0.0\n",
        "            for i, batch in enumerate(batch_iterator):\n",
        "                output = llm(**batch)\n",
        "                z = output.hidden_states[-1].to(torch.float32) # b, L, feature_dim\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                zhat, hz = sae.forward(z)\n",
        "\n",
        "                loss = sae.loss(z, zhat, hz, batch['attention_mask'])\n",
        "                total_loss += loss.item()\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                batch_iterator.set_postfix(mean_loss=total_loss / (i + 1), current_loss=loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dim = 2048 # 2048 for this Llama model\n",
        "sparse_dim = feature_dim * 8 # paper recommends 8-32x of feature dim for the SAE sparse dim\n",
        "alpha = 0.001 # hyperparameter, tune"
      ],
      "metadata": {
        "id": "P4NfUf_NcJm2"
      },
      "id": "P4NfUf_NcJm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a5b568d",
      "metadata": {
        "id": "2a5b568d"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "sae = SAE(feature_dim, sparse_dim, alpha, dtype=torch.float32).to(device=device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(sae.parameters())\n",
        "epochs = 2\n",
        "\n",
        "train(model, sae, dataloader, epochs, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the sae model weights\n",
        "torch.save(sae.state_dict(), 'llama-sae.pt')"
      ],
      "metadata": {
        "id": "sUNXbWmKMEcK"
      },
      "id": "sUNXbWmKMEcK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the sae model\n",
        "sae = SAE(feature_dim, sparse_dim, alpha, dtype=torch.float32).to(device=device)\n",
        "sae.load_state_dict(torch.load('llama-sae.pt', weights_only=True))"
      ],
      "metadata": {
        "id": "Z0kWuzlubi3H"
      },
      "id": "Z0kWuzlubi3H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sad_inputs = tokenizer('I am very sad and disappointed.', return_tensors='pt').to(model.device)\n",
        "happy_inputs = tokenizer('I am very happy and energetic.', return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    sad_outputs = model(**sad_inputs)\n",
        "    z = sad_outputs.hidden_states[-1].to(dtype=torch.float32)\n",
        "    sae.eval()\n",
        "    zhat, hz = sae.forward(z)\n",
        "    print(z.shape)\n",
        "    print(sad_inputs.input_ids)\n",
        "    print(torch.topk(z, 5, dim=2)[1])\n",
        "\n",
        "    happy_outputs = model(**happy_inputs)\n",
        "    z = happy_outputs.hidden_states[-1].to(dtype=torch.float32)\n",
        "    sae.eval()\n",
        "    zhat, hz = sae.forward(z)\n",
        "    print(z.shape)\n",
        "    print(happy_inputs.input_ids)\n",
        "    print(torch.topk(z, 5, dim=2)[1])\n"
      ],
      "metadata": {
        "id": "lxVPuuVpMd2A"
      },
      "id": "lxVPuuVpMd2A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpreting the model\n",
        "encoder_matrix = sae.E.enc.weight # 8 * 2048, 2048\n",
        "\n",
        "# Retrieve top k encoder weight values (corresponds to hidden features) per sparse feature\n",
        "k = 5\n",
        "topk_hidden = torch.topk(encoder_matrix, k, dim=1)[1]\n",
        "\n",
        "# Retrieve bottom k encoder weight values per sparse feature (for negative correlation)\n",
        "botk_hidden = torch.topk(encoder_matrix, k, dim=1, largest=False)[1]\n",
        "\n",
        "# Each row is a sparse feature, each value in the column are the top / bottom k hidden dimension indices that correlate to that sparse feature\n",
        "print(topk_hidden)\n",
        "print(botk_hidden)"
      ],
      "metadata": {
        "id": "JegRggh1S1IA"
      },
      "id": "JegRggh1S1IA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve top k samples for each sparse feature\n",
        "topk_samples = [[] for i in range(sparse_dim)]\n",
        "\n",
        "# instead of doing top k samples per feature, for each sample, pick top k of sparse features???\n",
        "with tqdm.tqdm(dataloader, desc=\"retrieving samples\", unit=\"batch\", total=len(dataloader), position=0, leave=True) as batch_iterator:\n",
        "    sae.eval()\n",
        "    total_loss = 0.0\n",
        "    for i, batch in enumerate(batch_iterator):\n",
        "        output = model(**batch)\n",
        "        z = output.hidden_states[-1].to(torch.float32) # b, L, feature_dim\n",
        "        _, hz = sae.forward(z) # b, L, sparse_dim\n",
        "\n",
        "        for b in range(4):\n",
        "            sentence = hz[b]\n",
        "            print(sentence.shape)\n",
        "            for d in range(sparse_dim):\n",
        "                avg_activation = torch.mean(sentence[:, d])\n",
        "                max_token_activation = torch.argmax(sentence[:, d])\n",
        "                if len(topk_samples[d]) < k:\n",
        "                    heapq.heappush(topk_samples[d], (avg_activation, max_token_activation, batch['input_ids'][b])) # error here: Boolean value of Tensor with more than one value is ambiguous, but unsure how\n",
        "                else:\n",
        "                    heapq.heappushpop(topk_samples[d], (avg_activation, max_token_activation, batch['input_ids'][b]))\n",
        "\n",
        "with open('topk-samples.pkl', 'wb') as f:\n",
        "    pickle.dump(topk_samples, f)"
      ],
      "metadata": {
        "id": "PhyFxtp1HpXJ"
      },
      "id": "PhyFxtp1HpXJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}